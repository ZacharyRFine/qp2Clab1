{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92439261",
   "metadata": {},
   "source": [
    "# QP 2 Computational Lab\n",
    "\n",
    "## Starter Code for Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9820806",
   "metadata": {},
   "source": [
    "The following cells are to set up your environment, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy opencv-python scikit-image matplotlib torch torchvision torchsummary albumentations cellpose captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63382e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import skimage\n",
    "import cellpose\n",
    "from torchsummary import summary\n",
    "\n",
    "def check_env():\n",
    "    print(\"--- Quantitative Physiology Lab: Environment Check ---\")\n",
    "    \n",
    "    # 1. Check Python Version\n",
    "    print(f\"Python Version: {sys.version.split()[0]} - {'OK' if sys.version_info >= (3,8) else 'UPDATE NEEDED'}\")\n",
    "\n",
    "    # 2. Check GPU Availability (Crucial for Week 3 & 5)\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"PyTorch GPU Acceleration: {'ENABLED (' + torch.cuda.get_device_name(0) + ')' if gpu_available else 'DISABLED (CPU Only)'}\")\n",
    "    if not gpu_available:\n",
    "        print(\"   NOTE: Training will be significantly slower without a GPU.\")\n",
    "\n",
    "    # 3. Check Image Processing Libraries\n",
    "    try:\n",
    "        test_img = (torch.rand(1, 1, 256, 256) * 255).numpy().astype('uint8')[0,0]\n",
    "        # Test OpenCV CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        _ = clahe.apply(test_img)\n",
    "        # Test Scikit-Image\n",
    "        _ = skimage.measure.label(test_img)\n",
    "        print(\"Image Processing Libraries (OpenCV/Skimage): OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"Image Processing Libraries: FAILED - {e}\")\n",
    "\n",
    "    # 4. Check CellPose API\n",
    "    try:\n",
    "        from cellpose import models\n",
    "        _ = models.CellposeModel(gpu=gpu_available, model_type='cyto2')\n",
    "        print(\"CellPose API: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"CellPose Check: FAILED - {e}\")\n",
    "\n",
    "    # 5. Check Albumentations\n",
    "    print(f\"Albumentations Version: {A.__version__} - OK\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c83a69",
   "metadata": {},
   "source": [
    "## Load a single image file\n",
    "\n",
    "Once your environment is correctly set up, choose one image from dominant_follicle set, load it as grayscale,  convert it to type float, and normalize to values between [0,1], using the starter code provided.  (JPG files are generally stored as 3-channel RGB files, even if the ultrasound looks black and white.  We load as grayscale to avoid doing calculations on three identical channels, which triples the memory usage for no reason.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_and_prep_ultrasound(path):\n",
    "    # 1. Load as grayscale even if the file is RGB\n",
    "    raw = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    raw = raw.astype(float) / 255.0\n",
    "\n",
    "    # 2. Check for empty file\n",
    "    if raw is None:\n",
    "        raise FileNotFoundError(\"Check your file path!\")\n",
    "    \n",
    "    return raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_prep_ultrasound(\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab93ece",
   "metadata": {},
   "source": [
    "## Filtering out Speckle Noise\n",
    "\n",
    "Time to deal with the Speckle. \n",
    "\n",
    "We might use a standard Gaussian blur, but that often \"smears\" the edges of the follicle, which is problematic for measurement. Instead, use a Median Filter. It is effective at removing salt-and-pepper noise while preserving the sharp edges of the follicle wall.  Apply your Median Filter here, using cv2.medianBlur( ).  \n",
    "\n",
    "As the argument to the filter function, choose an appropriate kernel size for the noise level of your image. Kernel size must be an odd positive integer. (Hint: If the goal is to remove sparse, high-contrast \"salt-and-pepper\" noise, even a small kernel like 3x3 or 5x5 is highly effective. A larger kernel will blur the image more, so you should use the smallest size that effectively removes the noise you are targeting.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0171b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd882a8",
   "metadata": {},
   "source": [
    "## Apply CLAHE\n",
    "\n",
    "Standard histogram equalization often over-amplifies noise. We will implement Contrast Limited Adaptive Histogram Equalization (CLAHE). CLAHE is advanced image processing technique that enhances local contrast by breaking images into small tiles (e.g., 8x8) and applying histogram equalization to each. \n",
    "\n",
    "You must choose parameters for the clip limit and tile grid size. \n",
    "\n",
    "The clip limit is a threshold for contrast amplification. Histograms in each tile are clipped at this limit to prevent noise over-amplification in relatively homogeneous regions of the image.\n",
    "\n",
    "Common effective values for the clip limit often fall within the range of 2 to 5. Values of 3 to 4 are frequently cited as generally effective.\n",
    "\n",
    "Lower clip limits (closer to 1) reduce noise but may result in insufficient contrast enhancement, producing an image closer to the original.\n",
    "\n",
    "Higher clip limits (e.g., in the range of 40 in some OpenCV implementations, or very large values) increase contrast but can also amplify noise.\n",
    "\n",
    "The optimal clip limit interacts with the tile grid size parameter. Experiment with both to achieve the desired results. The tile size should ideally be larger than the size of features you wish to preserve. \n",
    "\n",
    "CLAHE therefore limits contrast enhancement, specifically preventing noise amplification in homogeneous areas. Apply CHAHE to your filtered image using cv2.createCLAHE( ) with an appropriate clip limit and tile grid size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4e51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "978678cc",
   "metadata": {},
   "source": [
    "Here is some code for you to visualize your images. Experiment with different arguments in your median filter and CLAHE enhancement until you have what you believe to be the 'best' pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41892529",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Original', 'Median Filtered', 'CLAHE Enhanced']\n",
    "images = [img, img_median, img_clahe] # replace with your image variable names\n",
    "    \n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870a604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc141b84",
   "metadata": {},
   "source": [
    "## Determine outcomes of pre-processing\n",
    "\n",
    "We will now quantify the efficacy of our pre-processing steps by calculating the CNR, or Contrast-to-Noise Ratio, for our raw image, median filtered image, and median+CLAHE image. \n",
    "\n",
    "Use \n",
    "\n",
    "$CNR = \\frac{|\\mu_f-\\mu_s|}{\\sqrt{{\\sigma_f}^2+{\\sigma_s}^2}}$\n",
    "\n",
    "where $\\mu_f$ and $\\mu_s$ are the mean of the follicle and stroma regions of interest, and $\\sigma_f$ and $\\sigma_s$ are the standard deviations of the follicle and stroma regions of interest. \n",
    "\n",
    "Reminder: the follicle is a dark circular region on the image, and the stroma is the brighter, relatively homogenous connective tissue that surrounds the follicles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a6b24e",
   "metadata": {},
   "source": [
    "Here is some starter code that allows you to choose the ROI for the follicle and the stroma of an image. Use the *same ROI* for your calculations of CNR for your raw, median-filtered, and median+CLAHE images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f71bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select Follicle ROI\n",
    "print(\"Select Follicle, then press ENTER.\")\n",
    "roi_f = cv2.selectROI(\"Select Follicle\", img, False) # use your image name in place of img\n",
    "cv2.destroyWindow(\"Select Follicle\")\n",
    "\n",
    "# 2. Select Stroma ROI\n",
    "print(\"Select Stroma, then press ENTER.\")\n",
    "roi_s = cv2.selectROI(\"Select Stroma\", img, False)\n",
    "cv2.destroyWindow(\"Select Stroma\")\n",
    "\n",
    "# 3. Extract Data\n",
    "f_data = img[int(roi_f[1]):int(roi_f[1]+roi_f[3]), int(roi_f[0]):int(roi_f[0]+roi_f[2])]\n",
    "s_data = img[int(roi_s[1]):int(roi_s[1]+roi_s[3]), int(roi_s[0]):int(roi_s[0]+roi_s[2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139845b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339da7fc",
   "metadata": {},
   "source": [
    "Using the formula given above, calculate the CNR for your three images. When you are done, return to the lab manual for report instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f803bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fd871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
