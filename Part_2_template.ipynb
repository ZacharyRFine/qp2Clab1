{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58014d4",
   "metadata": {},
   "source": [
    "# QP2 Computational Lab Week 2 Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e82a35",
   "metadata": {},
   "source": [
    "In Part 1, we loaded our ultrasound images and did some pre-processing of an image to make the follicles more distinct, and therefore easier to recognize by AI models. We quantified the improvements to signal-to-noise ratio provided by a median filter and CLAHE method.\n",
    "\n",
    "This week, we will work to create 'masks' of the follicles on pre-processed images, which can be used for training AI systems to recognize our features of interest. \n",
    "\n",
    "We will explore three methods: \n",
    "* The Gold Standard: Manual annotation of images\n",
    "* A Silver (?) Standard: Using CellPose, a generalist algorithm for cellular segmentation, released in 2020\n",
    "* The 'Traditional' Method: Using OpenCV (cv2), an open-source computer vision library, available since 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1950227",
   "metadata": {},
   "source": [
    "First, let's pre-process all of the files in your Dominant Follicle folder. (No need to do the ROI or CNR checks anymore; we already validated this method in Part 1.). Here is some sample code for that. If you changed any of the processing parameters for median blur or CLAHE, make the same modifications here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f633ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm # Useful for showing a progress bar\n",
    "\n",
    "def bulk_process_ultrasound(input_dir, output_dir):\n",
    "    # 1. Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "    # 2. Get list of all .jpg files\n",
    "    image_paths = glob.glob(os.path.join(input_dir, \"*.jpg\"))\n",
    "    print(f\"Found {len(image_paths)} images to process.\")\n",
    "\n",
    "    # 3. Setup CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "\n",
    "    # 4. Loop through images\n",
    "    for path in tqdm(image_paths):\n",
    "        # Read image\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None: continue\n",
    "\n",
    "        # --- WEEK 1 PIPELINE ---\n",
    "        # Step A: Median Filter (Speckle removal)\n",
    "        denoised = cv2.medianBlur(img, 5)\n",
    "        \n",
    "        # Step B: CLAHE (Contrast stretch)\n",
    "        processed = clahe.apply(denoised)\n",
    "\n",
    "        # 5. Save the result\n",
    "        file_name = os.path.basename(path)\n",
    "        save_path = os.path.join(output_dir, file_name)\n",
    "        cv2.imwrite(save_path, processed)\n",
    "\n",
    "    print(f\"\\nDone! Processed images are in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdec921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1296 images to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [00:02<00:00, 445.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Processed images are in: output_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_directory = r'C:\\Users\\zacha\\qp2Clab1\\archive\\Ovarian_US\\dominant_follicle'\n",
    "bulk_process_ultrasound(input_directory, \"output_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c0eb8",
   "metadata": {},
   "source": [
    "Once this is done, you will set up LabelMe to generate high-quality \"Gold Standard\" datasets for medical segmentation. LabelMe is a Python-based annotation tool that allows us to draw precise polygons around follicles.\n",
    "\n",
    "Since LabelMe is a Python package, the safest way to install it is within a Virtual Environment to avoid conflicts with your other AI libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04231eef",
   "metadata": {},
   "source": [
    "# Manual Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53041220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a752dc",
   "metadata": {},
   "source": [
    "## Create a Dedicated Environment\n",
    "\n",
    "Open your terminal (or Anaconda Prompt) and run:\n",
    "\n",
    "\n",
    "conda create --name labelme python=3.9\n",
    "\n",
    "Then activate it:\n",
    "\n",
    "conda activate labelme\n",
    "\n",
    "## Install LabelMe\n",
    "\n",
    "Once the environment is active, install the package via pip:\n",
    "\n",
    "pip install labelme\n",
    "\n",
    "\n",
    "## Launch the App\n",
    "\n",
    "Simply type the name of the tool in your terminal:\n",
    "\n",
    "labelme\n",
    "\n",
    "A window should pop up. You can now go to File > Open Dir and select your folder of processed ultrasound .jpg files. Be sure to use the processed JPEGs, not the raw ones. LabelMe will create .json files in the same folder.\n",
    "\n",
    "## How to Annotate for Folliculometry\n",
    "To get the best results for your future U-Net, follow these \"Clinical Annotation\" rules:\n",
    "\n",
    "Use Polygons: Click \"Create Polygons\" (or press Ctrl+N). Do not use rectangles; follicles are organic, fluid-filled sacs and require precise boundary tracing.\n",
    "\n",
    "The \"Inner Wall\" Rule: In ultrasound, the follicle wall has some thickness. Aim for the interface where the dark fluid meets the gray tissue.\n",
    "\n",
    "Naming Conventions: When prompted for a label, use a consistent name like follicle. If you are tracking multiple follicles, you can name them follicle_1, follicle_2, etc.\n",
    "\n",
    "Save Often: LabelMe creates one .json file for every .jpg image. These files contain the coordinate points you will later convert into .png masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b17774",
   "metadata": {},
   "source": [
    "Your task: Annotate the dominant follicle in 5 images, using LabelMe.  When that is done, return here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eae99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b08d6b",
   "metadata": {},
   "source": [
    "Now that you have 5 .json files from LabelMe, let's do a Mask Audit. This makes sure our manual work is ready for AI. \n",
    "\n",
    "First, convert your json files to binary masks to create .png files. Here is sample code to do that one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def json_to_binary_mask(json_path, image_shape):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create blank canvas\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    \n",
    "    for shape in data['shapes']:\n",
    "        # Extract polygon points\n",
    "        points = np.array(shape['points'], dtype=np.int32)\n",
    "        # Fill polygon with 255 (Follicle)\n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "        \n",
    "    return mask\n",
    "\n",
    "# Save as PNG (NOT JPG!).  Example:\n",
    "# cv2.imwrite('mask_01.png', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b8b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shape is: (256, 256)\n"
     ]
    }
   ],
   "source": [
    "# if you're not sure what your image_shape is, check it here: \n",
    "import cv2\n",
    "\n",
    "# Load one of your processed images\n",
    "temp_img = cv2.imread('output_images/dominant_follicle_0001.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# This will give you a tuple like (480, 640)\n",
    "my_shape = temp_img.shape \n",
    "\n",
    "print(f\"The image shape is: {my_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97ea3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    json_path = f\"output_images/dominant_follicle_{i:04d}.json\"\n",
    "    mask = json_to_binary_mask(json_path, my_shape)\n",
    "    cv2.imwrite(f\"mask_{i:04d}.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea822ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment check completed successfully\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_mask_alignment(img, mask):\n",
    "    # Create a red-tinted overlay\n",
    "    overlay = img.copy()\n",
    "    overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
    "    overlay[mask > 0] = [0, 0, 255]\n",
    "    \n",
    "    # Blend the original and the overlay\n",
    "    alpha = 0.3\n",
    "    combined = cv2.addWeighted(overlay, alpha, cv2.cvtColor(img, cv2.COLOR_GRAY2RGB), 1 - alpha, 0)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "# Just process the first image without displaying\n",
    "img_path = f\"output_images/dominant_follicle_0001.jpg\"\n",
    "mask_path = f\"mask_0001.png\"\n",
    "\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if img is not None and mask is not None:\n",
    "    result = check_mask_alignment(img, mask)\n",
    "    print(\"Alignment check completed successfully\")\n",
    "else:\n",
    "    print(\"Could not load image or mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249950a",
   "metadata": {},
   "source": [
    "Now do an overlay check: Use the code below to overlay your mask onto the original image. If the red \"tint\" doesn't perfectly match the follicle boundary, you need to refine your trace in LabelMe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5020dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved alignment_check_0001.png\n",
      "Saved alignment_check_0002.png\n",
      "Saved alignment_check_0003.png\n",
      "Saved alignment_check_0004.png\n",
      "Saved alignment_check_0005.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "for i in range(1, 6):\n",
    "    img_path = f\"output_images/dominant_follicle_{i:04d}.jpg\"\n",
    "    mask_path = f\"mask_{i:04d}.png\"\n",
    "    \n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img is not None and mask is not None:\n",
    "        # Create a red-tinted overlay\n",
    "        overlay = img.copy()\n",
    "        overlay = cv2.cvtColor(overlay, cv2.COLOR_GRAY2RGB)\n",
    "        overlay[mask > 0] = [0, 0, 255]\n",
    "        \n",
    "        # Blend the original and the overlay\n",
    "        alpha = 0.3\n",
    "        combined = cv2.addWeighted(overlay, alpha, cv2.cvtColor(img, cv2.COLOR_GRAY2RGB), 1 - alpha, 0)\n",
    "        \n",
    "        # Save instead of display\n",
    "        cv2.imwrite(f\"alignment_check_{i:04d}.png\", combined)\n",
    "        print(f\"Saved alignment_check_{i:04d}.png\")\n",
    "    else:\n",
    "        print(f\"Could not load image or mask for follicle_{i:04d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960c215",
   "metadata": {},
   "source": [
    "# CellPose Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9434b",
   "metadata": {},
   "source": [
    "Unless you would like to manually annotate all of the files in the folder, let's try some ways to automate the rest of the dataset.  Cellpose was first made publicly available as a preprint on bioRxiv on February 3, 2020. The peer-reviewed paper was subsequently published in the journal Nature Methods on February 3, 2020, and appeared in the January 2021 issue. \n",
    "\n",
    "It was developed by Carsen Stringer, Tim Wang, Michalis Michaelos, and Marius Pachitariu of the Janelia Research Campus as a \"generalist algorithm for cellular segmentation\" designed to work on a wide variety of biological images. It's pretty impressive!\n",
    "\n",
    "NOTE: I do not expect Cellpose to do very well - don't be alarmed if it does a poor job.  But you might uncover a great way to use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0fa9d",
   "metadata": {},
   "source": [
    "Take a few minutes to go to cellpose.org and play around with the sample images there.  This will give you a lot better idea of the capabilities of the system and what we will be trying to do with our calls to Cellpose from our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4bd5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3829861195.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install cellpose\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# First install Cellpose\n",
    "pip install cellpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ce77f",
   "metadata": {},
   "source": [
    "Cellpose takes a lot of processing power. Before trying to annotate the entire folder, create a working folder of a subset; use just the files you manually annotated in LabelMe. Use the working folder for the rest of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73c2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Cellpose to annotate the small subset of your processed images, corresponding to those you manually annotated \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from cellpose import models, io\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_cellpose_v4_final(input_dir, output_dir):\n",
    "    # 1. Create directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 2. Setup Device (v4 standard)\n",
    "    device, gpu = models.assign_device(use_torch=True, gpu=True)\n",
    "    \n",
    "    # 3. Initialize ONLY the CellposeModel (Note the single .models)\n",
    "    # Using model_type='cyto2' here forces it to skip the SAM search\n",
    "    model = models.CellposeModel(gpu=gpu, pretrained_model='cyto2', device=device)\n",
    "\n",
    "    # 4. Filter for images\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "    print(f\"Processing {len(files)} images on {device}...\")\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        img_path = os.path.join(input_dir, f)\n",
    "        img = io.imread(img_path)\n",
    "\n",
    "        # 5. The V4-Clean Eval Call\n",
    "        # channels=[0,0] is used for single-channel grayscale\n",
    "        # invert=True is CRITICAL: it turns dark follicles into 'cells' for the AI\n",
    "        masks, flows, styles = model.eval(\n",
    "            img, \n",
    "            diameter=100,             # None = auto-estimate follicle size\n",
    "            channels=[0,0],         \n",
    "           \n",
    "           resample=True,\n",
    "           #invert=True,            # Must be done for ultrasound\n",
    "            #net_avg=True,           # Improves accuracy by running multiple passes\n",
    "            #cellprob_threshold=-2.0, # Higher sensitivity for faint follicles\n",
    "            #flow_threshold=0.4      # Standard flow; increase if masks are 'leaking'\n",
    "        )\n",
    "\n",
    "        # 6. Convert to binary mask (0 and 255)\n",
    "        binary_mask = (masks > 0).astype(np.uint8) * 255\n",
    "\n",
    "        # 7. Save as PNG\n",
    "        mask_name = f.replace('.jpg', '_mask.png')\n",
    "        cv2.imwrite(os.path.join(output_dir, mask_name), binary_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8be2cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretrained model C:\\Users\\zacha\\.cellpose\\models\\cpsam not found, using default model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 images on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]channels deprecated in v4.0.1+. If data contain more than 3 channels, only the first 3 channels will be used\n",
      " 20%|██        | 1/5 [00:15<01:03, 15.85s/it]channels deprecated in v4.0.1+. If data contain more than 3 channels, only the first 3 channels will be used\n",
      " 40%|████      | 2/5 [00:31<00:47, 15.91s/it]channels deprecated in v4.0.1+. If data contain more than 3 channels, only the first 3 channels will be used\n",
      " 60%|██████    | 3/5 [00:47<00:32, 16.04s/it]channels deprecated in v4.0.1+. If data contain more than 3 channels, only the first 3 channels will be used\n",
      " 80%|████████  | 4/5 [01:03<00:15, 15.74s/it]channels deprecated in v4.0.1+. If data contain more than 3 channels, only the first 3 channels will be used\n",
      "100%|██████████| 5/5 [01:18<00:00, 15.79s/it]\n"
     ]
    }
   ],
   "source": [
    "run_cellpose_v4_final(\"cellposeTest\", \"cellpose_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227047c",
   "metadata": {},
   "source": [
    "Compare a Cellpose-created mask to the manual mask of the same follicle. \n",
    "\n",
    "How did it do? If you did not get a good match, try adjusting the diameter variable and run again. Also try adjusting the cellprob_threshold and the flow_threshold. Keep track of the variables you adjust.\n",
    "\n",
    "Try (at least) three different parameter settings. Save the image corresponding to one of your manual masks for each parameter setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9f7da",
   "metadata": {},
   "source": [
    "## Evaluating the effectiveness of the Cellpose model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e678d2",
   "metadata": {},
   "source": [
    "To see how well Cellpose did, we need to calculate the Dice Coefficient (also known as the F1-Score). This metric is the industry standard for validating medical segmentations. It measures how many pixels the AI (Cellpose) and the Human (LabelMe) agreed upon, relative to the total number of \"follicle\" pixels.\n",
    "\n",
    "Dice Coefficient Evaluation Script\n",
    "\n",
    "This script will loop through your \"Gold Standard\" (manual) folder and find the corresponding Cellpose masks to calculate the overlap score.\n",
    "\n",
    "In medical imaging, a Dice score > 0.85 is generally considered excellent. If your Cellpose masks score significantly lower, it’s a sign that you need to adjust parameters in the model.eval function or do more \"Human-in-the-Loop\" corrections, meaning that you manually adjust poorly annotated images in LabelMe and return them to the folder.\n",
    "\n",
    "Dice= $ \\frac{2|A\\cup B|}{|A|+|B|}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2acbf6",
   "metadata": {},
   "source": [
    "How to Interpret Your Results\n",
    "\n",
    "\n",
    "|Dice Score|Interpretation\t|Action Required|\n",
    "| --- | --- | --- |\n",
    "|0.90 - 1.0\t|Excellent\t|The AI is ready to be used as ground truth for Part 3.|\n",
    "|0.75 - 0.89|\tGood\t|Standard for ultrasound; minor \"Human-in-the-Loop\" cleaning needed.|\n",
    "|Below 0.70\t|Poor|\tYour diameter parameter in Cellpose is likely wrong, or your CLAHE contrast is too low, or this method just isn't suitable.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b17115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name                     | Dice Score\n",
      "---------------------------------------------\n",
      "follicle_0001                     | 0.0007\n",
      "follicle_0002                     | 0.0000\n",
      "follicle_0003                     | 0.0000\n",
      "follicle_0004                     | 0.0010\n",
      "follicle_0005                     | 0.0113\n",
      "---------------------------------------------\n",
      "Mean Dice Score for Dataset: 0.0026\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def calculate_dice(manual_path, ai_path):\n",
    "    # Load masks as binary (0 or 255)\n",
    "    m_mask = cv2.imread(manual_path, cv2.IMREAD_GRAYSCALE)\n",
    "    a_mask = cv2.imread(ai_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if m_mask is None or a_mask is None:\n",
    "        return None\n",
    "\n",
    "    # Threshold to ensure they are strictly boolean (True/False)\n",
    "    m_bool = m_mask > 0\n",
    "    a_bool = a_mask > 0\n",
    "\n",
    "    # Calculate Intersection and Sum\n",
    "    intersection = np.logical_and(m_bool, a_bool).sum()\n",
    "    total_pixels = m_bool.sum() + a_bool.sum()\n",
    "\n",
    "    if total_pixels == 0:\n",
    "        return 1.0 # Both are empty, technically a perfect match\n",
    "\n",
    "    dice = (2. * intersection) / total_pixels\n",
    "    return dice\n",
    "\n",
    "def run_evaluation(manual_dir, ai_dir):\n",
    "    manual_files = glob.glob(os.path.join(manual_dir, \"mask_*.png\"))\n",
    "    scores = []\n",
    "\n",
    "    print(f\"{'Image Name':<30} | {'Dice Score':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    for m_path in manual_files:\n",
    "        # Extract the number from mask_0001.png -> 0001\n",
    "        base_name = os.path.basename(m_path).replace('mask_', '').replace('.png', '')\n",
    "        # Match to dominant_follicle_0001_mask.png\n",
    "        a_path = os.path.join(ai_dir, f\"dominant_follicle_{base_name}_mask.png\")\n",
    "\n",
    "        if os.path.exists(a_path):\n",
    "            score = calculate_dice(m_path, a_path)\n",
    "            scores.append(score)\n",
    "            print(f\"follicle_{base_name:<24} | {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Warning: AI mask not found for dominant_follicle_{base_name}_mask.png\")\n",
    "\n",
    "    if scores:\n",
    "        print(\"-\" * 45)\n",
    "        print(f\"Mean Dice Score for Dataset: {np.mean(scores):.4f}\")\n",
    "\n",
    "run_evaluation('manualMasks', 'cellpose_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual masks:\n",
      "  mask_0001.png\n",
      "  mask_0002.png\n",
      "  mask_0003.png\n",
      "  mask_0004.png\n",
      "  mask_0005.png\n",
      "\n",
      "AI masks:\n",
      "  dominant_follicle_0001_mask.png\n",
      "  dominant_follicle_0002_mask.png\n",
      "  dominant_follicle_0003_mask.png\n",
      "  dominant_follicle_0004_mask.png\n",
      "  dominant_follicle_0005_mask.png\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4aadf3a",
   "metadata": {},
   "source": [
    "# Traditional Computer Vision\n",
    "\n",
    "Cellpose was built primarily for fluorescence microscopy (sharp, glowing edges). Ultrasound is the opposite: it's \"speckle-heavy,\" noisy, and the boundaries are often fuzzy textures rather than clean lines.\n",
    "\n",
    "Let's try Traditional Computer Vision (Watershed Segmentation) or Active Contours. \n",
    "\n",
    "## Watershed Segmentation\n",
    "\n",
    "Watershed is the classic \"topographic\" approach. It treats your image like a map: dark follicles are \"valleys\" and bright stroma are \"peaks.\" We \"flood\" the valleys with water (labels) until they hit the peaks (walls). This uses OpenCV and doesn't require any pretrained models. It relies entirely on the contrast you created in Part 1.\n",
    "\n",
    "Test this on your subset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3e991f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 267.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def refined_watershed(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        # 1. Load and Blur\n",
    "        img = cv2.imread(os.path.join(input_dir, f), cv2.IMREAD_GRAYSCALE)\n",
    "        # A slight Gaussian blur helps merge speckle noise before thresholding\n",
    "        blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "        \n",
    "        # 2. Adaptive Thresholding\n",
    "        # Instead of one fixed number, this looks at 11x11 pixel neighborhoods\n",
    "        # Follicles are dark, so we use THRESH_BINARY_INV to make them white\n",
    "        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                      cv2.THRESH_BINARY_INV, 17, 7)\n",
    "\n",
    "        # 3. Morphological Cleaning\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        # 'Opening' removes white noise (speckles) from the background\n",
    "        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "        # 'Closing' fills small holes inside the follicles\n",
    "        closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "        # 4. Distance Transform (The \"Seed\" Creator)\n",
    "        # This calculates the distance from every white pixel to the nearest black pixel\n",
    "        dist_transform = cv2.distanceTransform(closing, cv2.DIST_L2, 5)\n",
    "        \n",
    "        # 5. Refining the Foreground (Sure FG)\n",
    "        # Increase the 0.2 factor to 0.5 if you get too many small detections\n",
    "        # Decrease it to 0.1 if follicles are being ignored\n",
    "        ret, sure_fg = cv2.threshold(dist_transform, .4 * dist_transform.max(), 255, 0)\n",
    "        sure_fg = np.uint8(sure_fg)\n",
    "\n",
    "        # 6. Save as Mask\n",
    "        mask_name = f.replace('.jpg', '_mask.png')\n",
    "        cv2.imwrite(os.path.join(output_dir, mask_name), sure_fg)\n",
    "\n",
    "\n",
    "refined_watershed('cellposeTest', 'watershed_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df98c1a",
   "metadata": {},
   "source": [
    "## Key Parameter Tuning\n",
    "\n",
    "If the output isn't perfect, these are the \"knobs\" you should turn (and document in your discussion):\n",
    "\n",
    "The Neighborhood Size (11, 2): In adaptiveThreshold, the 11 is the block size. If your follicles are very large, increase this to 21 or 31. If it's catching too much \"texture\" as follicles, increase the constant 2 to 5.\n",
    "\n",
    "The Distance Multiplier (0.2): This is the most sensitive part.\n",
    "\n",
    "If your follicles look like tiny dots: The multiplier is too high (e.g., 0.5). Lower it to 0.1.\n",
    "\n",
    "If multiple follicles are merged into one blob: The multiplier is too low. Raise it to 0.3 or 0.4.\n",
    "\n",
    "Without the distance transform, two follicles touching each other would be seen as one giant \"mega-follicle.\" The distance transform finds the \"peak\" (the center) of each follicle, allowing the algorithm to realize there are two distinct centers.\n",
    "\n",
    "Try (and document) three parameter adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(find og code and compare to the paramaters you have above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5ac67",
   "metadata": {},
   "source": [
    "Calculate a dice score for your best Watershed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8dbf1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name                     | Dice Score\n",
      "---------------------------------------------\n",
      "follicle_0001                     | 0.0181\n",
      "follicle_0002                     | 0.0502\n",
      "follicle_0003                     | 0.1289\n",
      "follicle_0004                     | 0.0299\n",
      "follicle_0005                     | 0.0150\n",
      "---------------------------------------------\n",
      "Mean Dice Score for Dataset: 0.0484\n"
     ]
    }
   ],
   "source": [
    "run_evaluation('manualMasks', 'watershed_masks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
